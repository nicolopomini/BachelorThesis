\chapter{State of the Art}
\label{cha:soa}

The identification of a life event into social media contents is a \textit{machine learning} problem, and to be more precise, a classification one. Machine learning is a field of computer science which deals with allowing a computer system to "\textit{learn with data, without being explicitly programmed}" \cite{samuel1959some}. It can be applied in many contexts, such as taking decisions, or make optimizations, forecasts and predictions. Nowadays a human being faces itself with machine learning in everyday life: home assistants, security surveillance, music and shopping suggestions, customer services are strongly powered by artificial intelligence. These services relies on data to learn how to work as good as possible: they are trained with samples of data similar to what they expect to receive by their users: the more accurate, exhaustive and in large quantities they are, the better the system learns. Therefore, data have a very central role in machine learning problems.

A classification task has the goal of assigning a belonging class to a given object. The input is composed by a tuple of \textit{features} that characterize the object, usually made by numbers, and the output is a categorical variable, such as a "yes/no" label. In other words, it can be seen as a mathematical function, that maps a vector $ \boldsymbol{x} \in \mathbb{R}^n $ to an answer $ y \in C $
\begin{gather*}
f \colon \mathbb{R}^n \to C \\
f \colon \boldsymbol{x} \mapsto y
\end{gather*}
where $C$ is a set of possible categories. A famous educational example of this kind of problem is the Iris flower classification\footnote{\url{https://en.wikipedia.org/wiki/Iris_flower_dataset}}, where the input $ \boldsymbol{x} $ is composed by the sepal and petal widths and lengths in cm of the flower, and $C = \{\text{\texttt{Iris setosa}, \texttt{Iris virginica}, \texttt{Iris versicolor}}\}$. For each sample composed by 4 measures the belonging class is predicted.

The background of live event identification on social media consists in a huge stream of documents, called \textit{posts}, each one written by a specific user and containing text, images, external links and other attachments, with a date of publication. Every user can share, comment or like someone else's post. A \textit{timeline} is the list of posts written by a single user, sorted by decreasing date of publication. These data are freely available on the Web (in some cases the author's authorization is necessary to access it) and contain many information. There is an endless number of works that use social media data for various purposes, and in this case they are used to detect events.

Event detection using social networks is a common practice. The literature offers many example of event detection on a global scale based on the analysis of social media contents, such as real-time earthquake identification based on tweets \cite{sakaki2010earthquake}, breaking news discovery in Twitter \cite{jackoway2011identification, phuvipadawat2010breaking}, or big gigs recognition observing what is posted on Flickr \cite{liu2011using}.

Event prediction using social network contents is also pretty common: the most striking example of the last few years are the 2016 american elections. In fact, while many of the official polls made by the most famous american newspapers and televisions had always forecasted Hillary Clinton as winner, social media reactions had been increasingly in favor of the Republicans during the election campaign\footnote{\url{https://techcrunch.com/2016/11/10/social-media-did-a-better-job-at-predicting-trumps-win-than-the-polls}}, and the rest is history. Other cases of event prediction using social media are, for example, movie box-office \cite{asur2010predicting} or Oscar-winner forecasts.

There is much less literature about life-event detection on social media, and it is almost completely focused on post classification. Another important issue about life events is related with data: it's hard to find it, and in a machine learning scenario like this problem data play a key role.

\section{Life event detectors on social media}
\label{sec:socialmediadetectors}
Almost every model proposed by the literature are classifiers that take a textual post as input and give as output a label, indicating whether the text talks about a life event. Each classifier is specialized to a single life event, for example a detector of posts about weddings. Some models are focused on a single post at a time, while few others \cite{cavalin2015multiple, moyanolife} consider also the conversation linked to the main post to understand the content and the importance of the speech. The feature extraction is done mainly in two ways: working with the text itself, using bag-of-words or bag-of-N-grams \cite{cavalinclassification, di2013detecting, li2014major}, or using a semantic analyzer, to obtain sentiment score, formality with which the text is written, entities and topics contained, etc \cite{khobarekar2013detecting}. Both methods are used combined together as well. In addition to that, some models consider also "external" features, like user's features (number of friends, age, location, post frequency, etc.) or post success \cite{dickinson2015identifying}.

All these models have several weaknesses for the purpose of this thesis, and they will be briefly described now. First of all, they are only classifiers that take into consideration a single post at the time labelling it, they don't go further with the information they obtain. Secondly, there is no consideration of the user timeline, of his/her behavior on social media, and no comparison with other posts published by the same user: in fact data is fetch more or less randomly from the social networks, without any user profiling intent, and consequently each post analyzed is completely disconnected with the other ones. Furthermore, analysing contents in this way there is no perception of how much the event may have lasted, and it is not understandable whether the event was in the past and now it's over, is just passed, it's occurring or it's coming in the near future. Another point worth of noting is that a post related to a life event may not be concerned with the author (e.g. participate to a wedding instead of getting married), or a post classified as about a life event could be a false positive: in both cases the decision to say that this person has lived a life event is based only on a single content, and this is a result of not consider the user's timeline. Thirdly, only text contents are considered: however, according to Mark Zuckerberg, CEO and founder of Facebook, "\textit{Most of the content 10 years ago was text, and then photos, and now it's quickly becoming videos}"\footnote{\url{https://www.fastcompany.com/3057024/mark-zuckerberg-soon-the-majority-of-content-we-consume-will-be-video}}. Not only are posts composed by more than only text, but many of them have no text at all. Furthermore, the meaning of an image or a video can give a strong clue to understand what the post is about: for example, an image containing a pink ribbon can easily be interpreted as a female baby announcement, without even look at the text. Last but not least, every model cited above uses Twitter contents only: this platform allows to share texts with at most 280 characters (until late 2017 the limit was 140) and it's used more for business purposes rather than personal life sharing. On the other hand, most of the contents on this social network are free to access and to obtain, without explicit user authorization.

\subsection{A step further}
The literature offers a model that goes a step further of those highlighted above: the work made by Cavalin, Gatti, Pinharez for the \textit{IBM InfoSphere BigInsights} platform \cite{cavalin2014towards}, starting from life event classification on a sample of tweets, it performs a user entity matching on an existing database of clients, aimed to decide which is the best approach to offer them some services or some products. Once a user that texts about a life event is identified, his/her information are used to match him/her into a database of clients.

In common with our model there is a big move forward from a simple classifier, but with an important difference: it solves the reverse problem of ours. Its goal is to get a list of users that posted about a life event in a given time window, while ours is to understand if a user has posted about a life event and in which time period. In this paper a wide range of posts written by many authors is taken to analyse the presence of a life event, not considering user's timeline at all, while our model is focused on a single user analysis. This brings all the problems highlighted above in the previous section. Furthermore, also this model is  concentrated only on Twitter contents.

\section{Other interesting methods}
In addition to the previous models, the literature offers several works that are methodologically interesting. They will be briefly described now.

As already mentioned, life event detection is a branch of event identification. The core of the business is therefore the identification of events from a stream of time-stamped documents coming from social networks. In concrete, the job made by Vavliakis, Symeonidis and Mitkas \cite{vavliakis2013event} organizes documents from various sources according to the event they describe, assigning to the event an importance, while the one made by CC Chen, MC Chen and MS Chen \cite{chen2009adaptive} tracks how much activity is related to an event from a global stream of documents. However, also these two models work on global scale, not analyzing a specific user, and they have an implicit issue: they consider the importance of the event based on the \textit{noise} the event brings with him. The more people talk about an event, the more important it is. Therefore every global event, such as The Olympics, will be classified as much more important than a wedding or a birth of a child.

Another interesting study is the one proposed by Li, Ritter and Jurafsky \cite{li2014inferring}, a model-driven system to infer user's attitudes or preferences reasoning over user's attributes and social network graphs. It builds, for every person taken into analysis, a series of predicates for his/her attributes (location, gender, education), relationships (married, friends) and preferences (what he/she likes/dislikes), which can be used to detect important events on his/her timeline or in friends/relatives social profiles. This work is interesting beacuse is the only one that is not completely driven by data and machine learning, but it offers a logical model, and also it combines multiple social networks (Twitter and Google+). On the other hand, of course, this model is not designed to deal with life events.

A last interesting study is about topic sentiment analysis using the hashtag graph in Twitter \cite{wang2011topic}. It demonstrates that hashtags offer additional information to texts, classifying tweets and users in categories: the use of a specific hashtag may be connected with a specific life event. This kind of study, however, has more sense on global scale analysis, like searching for users who show interest in a specific topic: it may be not so useful in a context of life event detection, which is concentrated on a single-user analysis.


\section{The dataset problem}
\label{sec:dataset}
Many users don't share with the world what happens to them in their private life, in fact most of the posts on social networks are not about users' personal life. There is also a strong subjectivity in sharing a life changing event: every user has his/her way to announce a personal news and it's hard to find a pattern that identifies a life event announcement. For these reasons is not easy to build a dataset big and reliable enough to train the system as good as possible. Furthermore, among the papers cited above, only one \cite{dickinson2015identifying} published the dataset build for the experiment\footnote{\url{http://reellives.net/rl-data/uploads/2015/06/a692044.csv}}.

The literature presents two different ways to fetch and label the data. Some authors prefer to fetch only contents that contain specific keywords (such as "\textit{engagement}" for marriage) \cite{dickinson2015identifying, khobarekar2013detecting}, labelling each content \textit{by hand} as about or not to the life event itself; some other search for contents randomly, labelling it in a more automatic way, considering a text related to the life event if it contains at least one keyword \cite{choudhury2014personal, di2013detecting, moyanolife}. The first method requires a bigger effort than the second one, because data is selected before the download, and once downloaded is analyzed by a human, who assigns to each post a relation with a life event. On the other hand, bigger work leads to better precision: a human classification is not based on the content only, but also on the meaning of the text. Of course this approach for huge datasets (those used into the cited papers with \textit{automatic} labelling space from tens of thousands to millions of samples) is not recommended. 

In case more than one life event is taken into consideration, this kind of classification becomes a multi-label classification. For example, if marriage and birth of a child are considered, a tweet that speaks about a wedding will have the \textit{marriage} label setted to \texttt{true}, and the \textit{having children} label setted to \texttt{false}. According to \cite{cavalinclassification} and to Wikipedia\footnote{\url{https://en.wikipedia.org/wiki/Multi-label_classification}}, the easiest way to perform this type of classification is doing a set of binary classification: to do that, a dataset for each life event is necessary. Another problem is represented by how many languages the classifier wants to support: for each of them is necessary a satisfying number of samples, to allow the classifier to learn any patterns or idioms related to each language. In conclusion, if the methodologies exposed by these works were followed, a dataset for each life event and for each language would be necessary. For example, a system that classifies posts about marriage and birth of a child written in english and italian, would need 4 datasets: marriage-english, marriage-italian, child-english and child-italian.

