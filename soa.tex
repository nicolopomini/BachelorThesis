\chapter{State of the Art}
\label{cha:intro}
Event detection using social networks is a common practice. The literature offers many example of event detection on a global scale based on the analysis of social media contents, such as real-time earthquake identification based on tweets \cite{sakaki2010earthquake}, breaking news discovery in Twitter \cite{jackoway2011identification, phuvipadawat2010breaking}, or big gigs recognition observing what is posted on Flickr \cite{liu2011using}.

Event prediction using social network contents is also pretty common: the most striking example of the last few years are the 2016 american elections. In fact, while many of the official polls made by the most famous american newspapers and televisions had always forecasted Hillary Clinton as winner, social media reactions had been increasingly in favor of the Republicans during the election campaign\footnote{https://techcrunch.com/2016/11/10/social-media-did-a-better-job-at-predicting-trumps-win-than-the-polls}, and the rest is history. Other cases of event prediction using social media are, for example, movie box-office \cite{asur2010predicting} or Oscar-winner forecasts.

There is much less literature about life-event detection on social media, most of the work done so far by the research is limited to classifiers for textual posts, expecially for Twitter contents. Another issue is about datasets: it's very hard to find enough reliable data to train and test properly a machine learning algorithm. In the following section both the research work and the dataset situation will be presented.

\section{Life event detectors}
\label{sec:classifiers}
Classification is the problem of assigning to an object never seen before a belonging class. This new object has a series of \textit{features}, a list of properties that characterize it, which are used by the classifier to decide the membership class. For example, in a text classification scenario, like this one, a possible list of features can be made by the length of the text, the words that compose it, how they are combined, their meaning and the order they appear. The decision is taken on the basis of a training set, a group of objects similar to the one into analysis, to whom the belonging class is known. 

The state of the art about life event detection on social media is focused on the develop of classifiers, that take a post as input and give as output a label, indicating whether that post speaks about a life event or not. All the models proposed in scientific papers and articles follow a similar pattern. Firstly, a set of life events to search for is fixed (e.g. getting married, birth of a child, buy a new house, etc.), then a dataset is build, fetching contents from social media and analyzing it, and finally a machine learning classifier is trained and tested on that data. This work is executed in several ways, in particular:
\begin{itemize}
\item Data is downloaded following two different paths. Some authors prefer to fetch only contents that contain specific keywords (such as "\textit{engagement}" for marriage) \cite{dickinson2015identifying, cavalinclassification, moyanolife, khobarekar2013detecting}, labelling each content \textit{by hand} as about or not to the life event itself; some other search for contents randomly, considering a text related to the life event if it contains at least one keyword \cite{choudhury2014personal, di2013detecting}. The first method requires a bigger effort than the second one, because data is selected before the download, and once downloaded is analyzed by a human, who assigns to each post a relation with a life event. On the other hand, bigger work leads to better precision: in fact using the first method is possible to know a priori which percentage of data is about life events, furthermore the labelling procedure is done with a better precision, not only relying on the content, but also on the meaning of the text.
\item Feature estraction is done mainly in two ways - working with the text itself, using bag-of-words or bag-of-N-grams \cite{cavalinclassification, di2013detecting, li2014major} - using a semantic analyzer, to obtain sentiment score, formality with which the text is written, entities and topics contained, etc \cite{khobarekar2013detecting}, or both methods. In addition to that, some models consider also "external" features, like user's features (number of friends, age, location, post frequency, etc.) or post success \cite{dickinson2015identifying}.
\item The majority of models consider only a single post at a time, while few others \cite{cavalin2015multiple, moyanolife} consider also the reactions and the conversation related to the main post.
\end{itemize}

For our purpose, these cited models have several weaknesses. The main issue is that they are limited only to classify a single post taken more or less randomly from a social network, there is no consideration of the user timeline, of his/her behavior on social media, and no comparison with other posts published by the same user. In fact a user who is preparing for a wedding may be concerned with sharing all the preparations and his/her feeling, so it's possible to find more than one post related to the same life event on a single timeline. Furthermore, analysing contents in this way, they are completely disconnected from each other: there is no perception of how much the event may have lasted, and it is not understandable whether the event is just passed, it's coming or it was in the past and now it's over. 

Another point worth of noting is that a post related to a life event may not be concerned with the author (e.g. participate to a wedding instead of getting married). In addition to that, a post classified as about a life event could be a false positive. In both cases there's no consideration about the user's timeline, and therefore the decision to say that this person has lived a life event is based only on a single content.

Last but not least, all these models are focused only on Twitter, and only on texts. This social network allows only short texts (up to 280 now, but until late 2017 the limit was 140) that might be written with abbreviations, and is mainly used for professional reasons, expecially in Europe.

\subsection{A step further}
The model described by Cavalin, Gatti, Pinharez for the \textit{IBM InfoSphere BigInsights} platform \cite{cavalin2014towards} goes a step further to all the previous classifiers. In fact, starting from life event classification on a sample of tweets, it performs a user entity matching on an existing database of clients, aimed to decide which is the best approach to offer them some services or some products. In other words, it solves the reverse problem of ours: its goal is to get a list of users that posted about a life event in a given time window, while ours is to understand if a user has posted about a life event and in which time period. 

To do that it applies fistly a slack word matching filter to download data from Twitter, then it uses more complex rules, like combination of words to make a further filter, and finally, using a machine learning classifier, it decides if a tweet is related to the life event, and consequently if the user is interesting. Once the user is detected, his/her information are used to match him/her into a database of clients.

In common with our model there is a big move forward from a simple classifier, but with an important difference: in this case a wide range of tweets from all over the world is taken to analyse the presence of a life event, not considering user's timeline at all, while our model is focused on a single user analysis. This brings all the problems highlighted above in the previous section. Furthermore, also this model is also concentrated only on Twitter contents. Anyway, the main issue is that the problem solved by this model is the inverse of ours.

\subsection{Other interesting methods}
In addition to the previous models, the literature offers several works that are methodologically interesting. They will be briefly described now.

As already mentioned, life event detection is a branch of event identification. The core of the business is therefore the identification of events from a stream of time-stamped documents coming from social networks. In concrete, the job made by Vavliakis, Symeonidis and Mitkas \cite{vavliakis2013event} organizes documents from various sources according to the event they describe, assigning to the event an importance, while the one made by CC Chen, MC Chen and MS Chen \cite{chen2009adaptive} tracks how much activity is related to an event from a global stream of documents. However, also these two models work on global scale, not analyzing a specific user, and they have an implicit issue: they consider the importance of the event based on the \textit{noise} the event brings with him. The more people talk about an event, the more important it is. Therefore every global event, such as The Olympics, will be classified as much more important than a wedding or a birth of a child.

Another interesting study is the one proposed by Li, Ritter and Jurafsky \cite{li2014inferring}, a model-driven system to infer user's attitudes or preferences reasoning over user's attributes and social network graphs. It builds, for every person taken into analysis, a series of predicates for his/her attributes (location, gender, education), relationships (married, friends) and preferences (what he/she likes/dislikes), which can be used to detect important events on his/her timeline or in friends/relatives social profiles. This work is interesting beacuse is the only one that is not completely driven by data and machine learning, but it offers a logical model, and also it combines multiple social networks (Twitter and Google+). On the other hand, of course, this model is not designed to deal with life events.

A last interesting study is about topic sentiment analysis using the hashtag graph in Twitter \cite{wang2011topic}. It demonstrates that hashtags offer additional information to texts, classifying tweets and users in categories: the use of a specific hashtag may be connected with a specific life event. This kind of study, however, has more sense on global scale analysis, like searching for users who show interest in a specific topic: it may be not so useful in a context of life event detection, which is concentrated on a single-user analysis.

\section{Dataset problem}
One of the greatest problem in this field is to find a good dataset upon which base a machine learning algorithm. On social network the majority of posts are not about user's personal life, and for this reason it's hard to create a dataset big enough to train the system as good as possible. Furthermore, only one of the papers cited above \cite{dickinson2015identifying} offers a public and freely available dataset\footnote{http://reellives.net/rl-data/uploads/2015/06/a692044.csv}.

Build a dataset from scratch requires a big effort and a lot of time: firstly, data must be downloaded, and some API (for example the Twitter ones) have a limited number of requests in a certain time window; secondly, data must be labelled, and this operation is very important for the quality of the classifier. These labels can be applied \textit{by hand} for a better precision, but for huge datasets (those used into the cited papers space from tens of thousands to milions of samples) this work is makable only automatically in a reasonable time.

In case more than one life event is taken into consideration, this kind of classification becomes a multi-label classification. For example, if marriage and birth of a child are considered, a tweet that speaks about a wedding will have the \textit{marriage} label setted to \texttt{true}, and the \textit{having chidren} label setted to \texttt{false}. According to \cite{cavalinclassification} and to Wikipedia\footnote{https://en.wikipedia.org/wiki/Multi\_label\_classification}, the easiest way to perform this type of classification is doing a set of binary classification: to do that, a dataset for each life event is necessary.

Another problem is represented by how many languages the classifier wants to support: for each of them is necessary a satisfying number of samples, to allow the classifier to learn any patterns or idioms related to each language.

In conclusion, if the methodologies exposed by these works were followed, a dataset for each life event and for each language would be necessary. For example, a system that classifies posts about marriage and birth of a child written in english and italian, would need 4 datasets. 

