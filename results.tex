\chapter{Implementation and evaluation}
\label{cha:results}
This chapter presents how a prototype of the system has been developed, giving a panoramic view of the interaction between components and a detailed focus on the solution chosen to deal with external services with a limited number of requests in a certain time range. After that, the development techniques for the classifier are presented, along with the performance evaluation of the latter one, and finally an evaluation about the detection process is provided.

For the evaluation, confusion matrices and some measures derived from them are used. A confusion matrix is a table layout used to show the results of a classification algorithm. Its columns represent the prediction output by the algorithm, while the rows represents the real class of the samples. In case of binary classifications, like those in this thesis, the matrix has 4 cells: the top-left one is a counter of the \emph{true positives} ($TP$) samples, the top-right for the \emph{false negatives} ($FN$), bottom-left for \emph{false positives} ($FP$), and finally bottom-right for \emph{true negatives} ($TP$). The sum of the four counters gives the number of samples analyzed. On these numbers, 4 measures are used:
\begin{equation*}
\begin{split}
\text{Accuracy} & = \frac{TP + TN}{TP + TN + FP + FN}\\
\text{Precision} & = \frac{TP}{TP + FP} \\ 
\text{Recall} & = \frac{TP}{TP + FN}\\
\text{F}_1\text{-Score} & = 2 * \frac{\text{Precision} * \text{Recall}}{\text{Precision} + \text{Recall}}
\end{split}
\end{equation*}

\section{Component interactions and communications}
\label{sec:view}

\begin{figure}
\centering
\includegraphics[width=%
0.7\textwidth]{img/Globalview}
\caption{A global view of the system with the data exchanged between the components. Grey arrows are queues of inputs, while white arrows are messages of a class method call.}
\label{fig:globalview}
\end{figure}

Overall, the whole system can be divided in two big parts: the one to get the data, the other to analyze them. The four logical steps previously explained in Section~\ref{sec:logicdescr} are translated into four components placed in the deepest points of each request, as shown in Figure~\ref{fig:globalview} and \ref{fig:interaction}. These four parts have different roles and behaviours: some of them are autonomous, while some others depend one from the other. Each one provides its own API, as shown in Section~\ref{sec:apis}, to allow the system to be scalable.

\begin{figure}
\centering
\includegraphics[width=%
1\textwidth]{img/Interaction}
\caption{The interaction among components over time.}
\label{fig:interaction}
\end{figure}

The request handler is the user's endpoint of the system, the only access point from the outside. This component allows managing the addition and modification of the users to be analyzed, and starts a content download or a computation of the timeline.

The download request will be described in detail in Section~\ref{sec:downloadqueues}, because due to the use of external services it is not immediate to explain.

The computation request is much easier, because it only works with data within the system, without any external service. Each request is fulfilled with the personal data of the user in analysis present at the moment of the request. This process can be seen in the lower part of Figure~\ref{fig:globalview}. User's posts are firstly classified as related or not to the life event taken into analysis, and then data is given to the timeline analyzer part to detect the life events into user's timeline, as explained in Section~\ref{sec:computationperformer}. 

\subsection{A queue system to download user's contents}
\label{sec:downloadqueues}

\begin{figure}
\centering
\includegraphics[width=%
1\textwidth]{img/DownloadStatuses}
\caption{The changing of status over time of a download request.}
\label{fig:statuses}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=%
0.65\textwidth]{img/Queues}
\caption{The route of a download request. From the endpoint, data is divided in independent streams, one for each social media, and each stream is enqueued in the corresponding queue. The red, green and purple frames represents three different social network download system -- for example the red one for Facebook, the green one for Twitter and the purple one for Instagram. Once data is downloaded, it is saved into the database, and posts are given to the semantic analyzers, represented by the orange and blue frames: one for texts and one for images.}
\label{fig:queues}
\end{figure}

The most complex request is a \texttt{/downloadrequest}\footnote{For further details about the request endpoints, visit the official API: \url{lifeevents.docs.apiary.io}}, because it has to face itself with external APIs over HTTPS, which have free plans with some request limits over a time range. Due to this issue, each part needed to fulfill this kind of requests has to be able to suspend itself without interrupt the whole system, in case request number runs out. In other words, when the function for downloading contents on a social network, or the one that deals with the external semantic analyzer, reaches the limit of requests, it has to wait for another time slot stopping only itself and nothing else. For this reason data is not passed with a standard class method call, but it is inserted into a queue. In this way the component that asks for an external service does not need to care if the downloader or the semantic analyzer are already busy or suspended. There will be a queue for each module that has to interface itself with an external service, so one for each social media and one for every semantic analyzer (one for texts and one for pictures). This decision is necessary to avoid bottlenecks as much as possible: in case a social network API reaches the limit, the downloads in the other social networks can be carried on, and so also the following requests can start. The only thing that is necessary to track is when each part of the Download Request finishes, in order to have always a current status for every request of download: when at least a data stream of a request is taken over by some downloader or analyzer, the status is \emph{in progress}, while if all data streams of a requests are waiting into a queue, the status is \emph{waiting}. By the time each stream passed into its specific downloader and analyzer successfully, the status is \emph{completed}. The status timeline can be seen in Figure~\ref{fig:statuses}.

For example, let us suppose to download contents made by a user who is signed up to Twitter, Facebook and Instagram. The request is collected by the API endpoint, which verifies if the user exists into the database of the system, and in case it exists the endpoint gets from the same database, for each of the three social media, the user ID on that social network and the last post downloaded in a previous request, if there is one. After that, the tuple (user ID, starting point, number of post to download) is queued in the respective social media queue, and an immediate response is given to the API request, to let the user know that her request has taken into consideration. At this point, three streams of data -- one for each social media -- are created, and they go on independently: each of them waits its turn into the downloader queue, then data is downloaded and saved into the database. At this point other two streams for each downloaded post are created, one for images and one for texts: the tuples (post ID, text) and (post ID, photos) are enqueued in the text semantic analyzer and in the image semantic analyzer respectively. When all these streams pass both downloader and analyzers, the request is \emph{completed}. This schema can be seen in Figure~\ref{fig:queues}.

Even though this architecture avoids as much as possible bottlenecks, a download request can take a very long time to be completed, even days, because each post from any social media has to pass through the semantic analyzers, which have a very restricted number of requests a day.

\section{Implementation choices}
\label{sec:choices}
Due to the recent Cambridge Analytica scandal that involved Facebook, and with the consequent policy updates made by the social network, develop an application that works with the products of this latter (Facebook itself and Instagram, for instance) that uses user's contents has become difficult. To do that, Facebook has to release a manual authorization for the app, and the process to obtain it requires a demo video to show what the application does, and a detailed description of how user data is used. Even if a beta development had been used, it would have been hard to find enough users available to share their data for the development of this system. For these reasons it was decided to base the prototype on Twitter.

Another point worth of noting is that the developed software is only a demo: in fact, the use of social network data for commercial purposes without explicit user authorization violates the terms of use of the latter. Furthermore, the processing of personal data in Europe is regulated by the General Data Protection Regulation (GDPR), and the use of those data must be authorized by the owner of them (the author herself), for example via \emph{social login}.

Another issue is related with the external semantic analyzers: only a free account has been used, which implies a very strict number of requests per day (1000). This means that currently a computation can take a very long time to be executed, even days, because each post has to be analyzed by this external services. Furthermore, image analyzers have free plan with only 1000 request a month, so it was decided to use only texts of posts, and to ignore images. All the obtained result, therefore, are only related with texts of tweets. On the other hand, some tests with images were executed, showing that the use of images would give better results: in fact, several posts of life events are composed only by images, so at the current situation they cannot be classified correctly.

About the implementation, the prototype of the system is developed in Python, using some standard libraries for machine learning and mathematical purposes: the classifiers are those available in the \texttt{scikit-learn}\footnote{\url{scikit-learn.org}} library \cite{scikit-learn}, while for numerical arrays, matrices and other mathematical functions the \texttt{NumPy}\footnote{\url{www.numpy.org}} library \cite{oliphant2006guide} is chosen. Data is stored and organized using a \texttt{MongoDB} database\footnote{\url{www.mongodb.com}}. Last but not least, the external semantic analyzer for texts is called \texttt{Dandelion}\footnote{\url{dandelion.eu}} made by SpazioDati S.r.l.\footnote{\url{spaziodati.eu}}. Some test with images were also performed, using the \texttt{Cloud Vision API} of Google Cloud\footnote{\url{cloud.google.com/vision}}.

\section{Feature extraction}
\label{sec:featurextraction}
In this section the feature extraction technique is presented. This method is thought to work with posts from various social networks, whose structures are standardized as can be seen in Section~\ref{sec:apis}. In general, each post has a text and a series of images that are used for the classification. According to Section~\ref{sec:dataset}, Wikipedia entities are used as features, for two reasons:
\begin{enumerate}
\item In this way, it is possible to work with as many languages as is wanted, using only one for training. In fact, letting $ L $ be the language of the samples in the training set, it is enough to map all the entities found in posts written in a language $ L^{'} \ne L $ to the corresponding entity in the $ L $ version of Wikipedia. All the most common entities for life events exist in many version of the online encyclopedia. For this reason, this method allows a great scalability regarding the languages supported.
\item With this method is possible to consider texts and images in the same way, seeing them as \emph{entity containers} without differentiating them.
\end{enumerate}

The feature extraction is done in this way: each entity of the samples of the training set is kept as feature, and a matrix $X$ is created, with $n$ rows, one for each sample to classify, and $m$ columns, one for each feature. Each element [$i,j$] of $X$ is the counter of how many times the entity $j$ is found in the post $i$. The number $m$ of columns is in the order of thousands.

Of course, the features used for weddings are different from those used one for births, and consequently two different machine learning classifiers are needed, each one trained for its specific life event. To do that, the dataset taken from \cite{dickinson2015identifying} was filtered, keeping only tweets about a wedding or a birth of a child. Then it was splitted in two files, one for each life event, and every tweet was enriched with Wikipedia entities and topics using the external semantic analyzer. In the end, the wedding dataset counted $2538$ samples, the births one $2233$. Each of these sets were very balanced, with half of the sample related to the life event and the other half not. Both datasets were then used to train each classifier.

\section{Construction and performance evaluation of the classifier}
In this section all the techniques used to create the classifier are explained.

 According to the literature, a naive bayes classifier and a decision tree were chosen as classifiers, but due to the better performance of the first one, the second one was initially discarded. In particular, the classifier was a Multinomial Naive Bayes\footnote{\url{scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes}}.

The very first feature extraction technique was inspired by the \texttt{tf-idf} weight function\footnote{\url{en.wikipedia.org/wiki/Tfâ€“idf}}, which is used to understand the importance of a word into a series of documents. The idea was to select a small sample of key entities analyzing some web pages focused on the life event taken into consideration, such as a wedding planner homepage or a pregnancy blog. These $k$ top entities were used as features of tweets, and a matrix $X$ was created in the same way explained in Section~\ref{sec:featurextraction}, but with $m = k$. However, this methodology performed worse than a random classifier, with both precision and recall scores lower than $0.5$, with $k = 3, 4, 5, 10, 100$.

\begin{table}[htbp]
\centering
\subtable[Results on a single dataset split\label{tab:singlesplit}]{%
\begin{tabular}{cccc}
\hline
Life event & Precision & Recall & F1 Score \\
\hline
\textbf{Getting married} & $0.82$ & $0.81$ & $0.81$ \\
\textbf{Having children} & $0.77$ & $0.78$ & $0.77$ \\
\hline
\end{tabular}
}\qquad\qquad
\subtable[Results on a 10 fold cross validation\label{tab:kfold}]{%
\begin{tabular}{cccc}
\hline
Life event & Precision & Recall & F1 Score \\
\hline
\textbf{Getting married} & $0.73$ & $0.82$ & $0.77$ \\
\textbf{Having children} & $0.62$ & $0.57$ & $0.59$ \\
\hline
\end{tabular}
}
\caption{The performance of the naive bayes were satisfactory on a very balanced dataset, but not with an unbalance situation. Unfortunatly, users' timelines are very unbalanced, and this classifier turned out to be inappropriate.}
\end{table}

A much better performance with the dataset, but unfortunately not with the reality, was given by using all the Wikipedia entities of the training set as features, exactly as explained in Section~\ref{sec:featurextraction}. Firstly, the dataset was splitted into a training and a test set, the first one with the 70\% of the samples and the second with the remaining 30\%; secondly, the matrix $X$ was created. On the test set this solution had good results, as shown in Table~4.1(a) and~4.1(b), but in the reality, trying to predict if tweets from a user timeline were about or not a life event, this solution turned out to be impractical: the discrimination was too loose, and many tweets that was not related with the life event at all were considered "positives", with the precision measure that fell dramatically under $0.1$. The overfitting reason was excluded, because the training set was kept aside from everything else. In addition, each sample in the dataset was written by a unique user. The most probable reason is that the dataset used for the training was too balanced compared to the real situation in social networks: in fact, while the dataset contained about 50\% of tweets about a life event and 50\% not, a timeline had a very small percentage of contents about a life event. For example, analyzing a portion of Gareth Bale's Twitter profile\footnote{\url{twitter.com/GarethBale11}}, it turned out that on 224 tweets taken into consideration only 2 were about the birth of a child and only one about marriage. Many other profiles considered had similar proportions.

A third approach was tried, with the purpose of improving the previous situation: train the classifier with a very unbalanced dataset, discarding randomly many of the positive samples to create a 90\% negatives and 10\% positives situation. As before, the results were good only with the test set: with a real profile the discrimination was too strong this time, not finding any tweet at all, maybe just because of this disproportion, where too few related samples were given and so the classifier did not see enough entities among those of positive samples.

Another very big issue was caused by the external semantic analyzer - Dandelion - used to discover Wikipedia entities inside texts. Its results turned out to be very inaccurate for such short texts written in Twitter: for example, a tweet published by a famous sports man about his wedding, with the text "\textit{This is her, my wife}"\footnote{\url{twitter.com/petosagan/status/667069250268479488}} was mapped by the analyzer to a song by the British rock band \emph{the Who} called \emph{My Wife}, not understanding the context at all. Some other cases of misunderstanding between the text and its true meaning were found, for example the word \emph{"congratulations"} was mapped into the entity "\texttt{Congratulations: 50 Years of the Eurovision Song Contest}", but this was not a big issue because this combination was always respected every time that word was found. Just to be sure that some words were recognized correctly, 4 keywords for each event were manually mapped into their correct entities, such as \emph{baby} on the entity \texttt{Infant}.

\begin{sidewaysfigure}
\centering
\includegraphics[width=%
1\textwidth]{img/decisiontree}
\caption{The decision tree for the birth of a child event. As can be seen, the nearest nodes to the root are decisions about entities that are strictly connected with the life event itself, and each of them makes a significant split in terms of the size of the resulting subsets, while the deepest nodes are strongly connected with the training set. For this reason the maximum depth was forced to be low.}
\label{fig:decisiontree}
\end{sidewaysfigure}

To avoid issues with the semantic analyzer as much as possible, as few as possible entities were used to classify a tweet: a very first approach consisted in a manual selection of features, for example the entity \texttt{Wife} for wedding, but the results were not satisfactory. The definitive approach was to let the data decide which entities to use, relying on the dataset taken from \cite{dickinson2015identifying}. The classifier was changed, opting for a decision tree, because the previous naive bayes was too loose during the classification. A decision tree is a machine learning model used for regression and classification based on a binary tree data structure. Each node of the tree is a question relative to a feature of the objects to classify, and each leaf represent a decision, an estimation of the value of the input object. During the training, the tree is built starting from the root: at each step of the construction, the goal is to split the dataset into two, making a question on a feature of the objects, to reduce the unpredictability of the two new subsets as much as possible. Every object in input has its own path from the root to a single leaf, which represents the decision. In this case each leaf contains a probability with which a sample that ends its path there is about the life event. In this way, there is a finite set of probability that are given to each post in input. The classifier is the decision tree offered by the \texttt{scikit-learn} library\footnote{\url{scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html}}. To reduce the overfitting on the training data, the maximum depth of the tree is limited to 5, so for each root-leaf path, at most 4 intermediate nodes could be found. The decision tree for the \texttt{Having Children} life event can be seen in Figure~\ref{fig:decisiontree}.

To further improve the results and to make sure that some key entities are identified, an automatic check on the texts of the posts is done. In particular, for each life event $LE$ and for each language $L$, a set of keyword translation $T(LE, L)$ is defined. For example, for the life event \texttt{Having children} and for the english language, $T$ is so defined:
\begin{equation*}
\begin{split}
T(\text{\texttt{Having children}}, \text{english}) = \{&(\text{"birth"}, \text{ \texttt{\url{http://en.wikipedia.org/wiki/Birth}}}),\\&(\text{"born"}, \text{ \texttt{\url{http://en.wikipedia.org/wiki/Birth}}}),\\&(\text{"baby"}, \text{ \texttt{\url{http://en.wikipedia.org/wiki/Infant}}}),\\&(\text{"child"}, \text{ \texttt{\url{http://en.wikipedia.org/wiki/Child}}})\}
\end{split}
\end{equation*}

The results with the dataset of \cite{dickinson2015identifying} were a little worse than those of the bayesian classifier shown in Tables~4.1(a) and~4.1(b), but in the reality, with a test conducted on 5836 tweets taken by 44 different timelines the results were much better than the previous one, as explained in Tables~4.2(a) and~4.2(b).

\begin{table}[h]
\centering
\subtable[Results about wedding tweets\label{tab:treewedding}]{%
\begin{tabular}{ccccc}
\hline
Sample & Precision & Recall & F1 Score & Support \\
\hline
\textbf{True} & $0.87$ & $0.70$ & $0.77$ & $134$ \\
\textbf{False} & $0.91$ & $0.97$ & $0.94$ & $5702$ \\
\textbf{Avg / Total} & $0.90$ & $0.90$ & $0.90$ & $5836$ \\
\hline
\end{tabular}
}\qquad\qquad
\subtable[Results about birth of a child tweets\label{tab:treechild}]{%
\begin{tabular}{ccccc}
\hline
Sample & Precision & Recall & F1 Score & Support \\
\hline
\textbf{True} & $0.65$ & $0.62$ & $0.63$ & $161$ \\
\textbf{False} & $0.93$ & $0.94$ & $0.94$ & $5675$ \\
\textbf{Avg / Total} & $0.89$ & $0.89$ & $0.89$ & $5836$ \\
\hline
\end{tabular}
}
\caption{The performance of the decision tree with a very unbalanced dataset. On this same test set the previous classifier, the naive bayes, showed both precision and recall scores under 0.1.}
\end{table}

\section{Detection evaluation}
\label{sec:detectioneva}
\noindent
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}
\begin{table}
\centering
\subtable[Getting married\label{tab:weddingconf}]{
\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
  \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft Actual\\ value}} & 
    & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
  & & \bfseries True & \bfseries False & \bfseries Total \\
  & True & \MyBox{4}{} & \MyBox{4}{} & 8 \\[2.4em]
  & False & \MyBox{0}{} & \MyBox{22}{} & 22 \\
  & Total & 4 & 26 & 30
\end{tabular}
}
\quad
\subtable[Having children\label{tab:childrenconf}]{
\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
  \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft Actual\\ value}} & 
    & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
  & & \bfseries True & \bfseries False & \bfseries Total \\
  & True & \MyBox{12}{} & \MyBox{9}{} & 21 \\[2.4em]
  & False & \MyBox{0}{} & \MyBox{9}{} & 9 \\
  & Total & 12 & 18 & 30
\end{tabular}
}
\caption{Confusion matrices of boolean tests}
\end{table}
Before presenting the results about the detection method it is right to make a premise. The results that will be showed now are obtained using data that are composed only by texts coming from Twitter. The reason why Twitter was used is because it has the easiest policy for download authorization: while on Facebook is necessary the explicit permission of both the social network and the user for downloading and using personal data, Twitter allows to use it for non-commercial purposes just registering a developer account. In addition to that, images were not used because semantic analyzer services for photos allow only 1000 requests a month freely, so it was not computationally possible to process thousands of posts with images. Some isolated tests with images was tried, and posts about life events that had not been classified correctly using their texts were correctly by their photos.

First of all, the tests were executed with 30 different Twitter accounts, some of them completely downloaded, some other only partially. Totally, 21 users had children in their real life and shared something about it, 8 got married and shared something too, six of which lived both a marriage and a birth of a child. In addition to that, 7 other accounts that were not concerned with these two life event were added to the samples. The reason why only 30 account were downloaded (and some of them partially) is always because of semantic analyzer free plans: in fact \texttt{Dandelion} offers 1000 requests a day for free, and for each post two requests were necessary: one for Wikipedia entities, and one for the sentiment score. Thus, the necessary machine time to download completely an account could be greater than a week.

Secondly, the parameters setted for the detection algorithm, whose meanings can be seen in Section~\ref{sec:computationperformer}, were the following: $p_1 = 0.6$, $p_2 = 0.4$, $\alpha = 0.0$, $ \beta = 2$, $ \gamma = 182$.

Thirdly, two types of tests were made:
\begin{itemize}
\item A boolean valuation without consider the temporal factor, that indicates whether the detection reflects the fact that the subject of the analysis really experienced that event in a some time in his life.
\item An evaluation on the quality of the time ranges returned.
\end{itemize}
For the first test, which is the classic test usually done with a classification problem, the results can be seen in the two confusion matrices, 4.3(a)~for weddings and 4.3(b)~for births respectively. Many \emph{false negative} samples may be due to the fact that the timeline analysis algorithm considered a life event only if at least two posts about it were met. The reason is to avoid as much as possible cases in which the user talked about the life event by chance, for example posting about a movie in which there is a marriage. Another possibility is related to the exclusive use of texts in these tests.

For further details, the following are the results for wedding cases,
\begin{gather}
Accuracy = 0.87 \quad Precision = 1.0 \quad Recall = 0.50 \quad F_1\text{-}Score = 0.67
\label{weddingconfdata}
\end{gather}
and for births of child
\begin{gather}
Accuracy = 0.70 \quad Precision = 1.0 \quad Recall = 0.57 \quad F_1\text{-}Score = 0.73
\label{childrenconfdata}
\end{gather}
respectively.

\setlength{\tabcolsep}{9pt}
\begin{table}[htbp]
\centering
\subtable[Time ranges about weddings\label{tab:rangeswedding}]{%
\begin{tabular}{cccc}
\hline
\multicolumn{2}{c}{\textbf{Detection result}} & \multirow{2}*{\textbf{Ground Truth}} & \multirow{2}*{\textbf{Correctness}} \\
\textbf{From} & \textbf{To} & \\
\hline
22/09/2015 & 22/09/2015 & 03/10/2015 & False \\
$^{(7)}$09/08/2016 & 09/08/2016 & 07/11/2015 & False \\
$^{(3)}$14/05/2016 & 13/06/2016 & 13/06/2016 & True \\
$^{(4)}$15/05/2018 & 20/05/2018 & 15/05/2018 & True \\
\hline
\end{tabular}
}\qquad\qquad
\subtable[Time ranges about births of a child\label{tab:rangeschild}]{%
\begin{tabular}{cccc}
\hline
\multicolumn{2}{c}{\textbf{Detection result}} & \multirow{2}*{\textbf{Ground Truth}} & \multirow{2}*{\textbf{Correctness}} \\
\textbf{From} & \textbf{To} & \\
\hline
$^{(5)}$13/05/2018 & 13/05/2018 & 13/05/2018 & True \\
$^{(6)}$17/03/2016 & 17/03/2016 & 20/07/2016 & False \\
25/10/2017 & 17/11/2017 & 26/10/2017 & True \\
07/11/2015 & 18/11/2015 & 18/12/2015 & False \\
15/01/2015 & 15/01/2015 & 15/01/2015 & True \\
22/03/2016 & 22/03/2016 & 22/03/2016 & True \\
$^{(2)}$09/03/2018 & 08/05/2018 & 08/05/2018 & True \\
05/01/2012 & 05/01/2012 & 05/01/2012 & True \\
14/07/2011 & 14/07/2011 & 14/07/2011 & True \\
22/12/2016 & 22/12/2016 & 22/12/2016 & True \\
16/10/2016 & 16/10/2016 & 01/04/2017 & False \\
$^{(1)}$04/01/2017 & 26/08/2017 & 30/07/2017 & True \\
\hline
\end{tabular}
}
\caption{The list of tests made on \emph{true positive} samples. The first two columns represent the time range outputted by the detection, the third is the real date of the event, and the last one indicates if the result is correct or not. The numbered results are graphically represented in Figure~\ref{fig:timeranges}.}
\end{table}

\setlength{\tabcolsep}{6pt}
\begin{table}
\centering
\begin{tabular}{cccc}
\multicolumn{4}{c}{\textbf{Correct intervals}} \\
\hline
\textbf{Life event} & \textbf{Length} (days) & \textbf{\emph{Left} space} (days) & \textbf{\emph{Right} space} (days) \\
\hline
\multirow{2}*{\texttt{Getting married}} & 31 & 30 & 0 \\
& 6 & 0 & 5 \\
\hline
\multirow{9}*{\texttt{Having children}} & 1 & 0 & 0 \\
& 24 & 1 & 22 \\
& 1 & 0 & 0 \\
& 1 & 0 & 0 \\
& 61 & 60 & 0 \\
& 1 & 0 & 0 \\
& 1 & 0 & 0 \\
& 1 & 0 & 0 \\
& 235 & 207 & 27 \\
\hline
\end{tabular}
\caption{Details about the correct intervals. \emph{Left space} indicates the number of days between the starting point of the interval and the real date of the event, while the \emph{right space} is the number of days from the event to the end of the interval.}
\label{tab:correctintervals}
\end{table}

\begin{table}
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{\textbf{Wrong intervals}} \\
\hline
\textbf{Life event} & \textbf{Distance} (days) \\
\hline
\multirow{2}*{\texttt{Getting married}} & 11 \\
& -276 \\
\hline
\multirow{3}*{\texttt{Having children}} & 125\\
& 27 \\
& 167 \\
\hline
\end{tabular}
\caption{Details about the wrong intervals. The distance is the difference in days between the date of the real event and the nearest extreme of the interval. Positive values indicate that the detected interval finishes before the ground truth of the event, while a negative value indicates that the time range starts after the date of the event.}
\label{tab:wrongintervals}
\end{table}

\begin{sidewaysfigure}
\centering
\includegraphics[width=%
1\textwidth]{img/Timeranges}
\caption{A graphical representation of the results in Table~4.4(a) and Table~4.4(b). In each time line, composed by 364 days (52 weeks), the light blue slice is the output of the detection, while the red vertical bar is the date of the real event. Case~(1) shows a very large but correct detection, cases~(2) and~(3) are detections in the order of magnitude of the month, cases~(4) and~(5) are the most precised detections, with a length within a week. Finally, cases~(6) and~(7) are wrong detection. As can be seen in the correct detections, the ground truth stays very near to an extreme of the interval: this depends on how the user behaves in sharing news about her private life. When the the date of the real event is near to the right extreme of the interval, this means that the user announced the live event some time before its happening, while contrariwise if the event is near the left extreme, it means that the user preferred to share something later the event. The first pattern is the most common, expecially with births of child, while the second one is rarer, used sometimes with weddings.}
\label{fig:timeranges}
\end{sidewaysfigure}

The second type of test was done only on the \emph{true positive} samples of the first test, that are those samples that lived a \emph{life event} in their real life and for whom a time range was returned by the algorithm. Firstly, the correctness of the time ranges compared to the ground truth of the event was checked, as can be seen in Table~4.4(a) for the weddings and in Table~4.4(b) for the births. Then, for the correct results the length and the "displacement" of the intervals were analyzed, while for the wrong detection the error expressed in number of days were reported.

In particular, the results about correct intervals are shown in Table~\ref{tab:correctintervals}: as can be seen, the most of the intervals are shorter than a week, this because users tend to speak about life events when they are very close to leave them. Furthermore, 6 users posted about the birth of their child only the day the event happened, for this reason the beginning and the end of the interval are overlapped. 

The intervals that last for weeks or months indicate that the user decided to announce the life event some time before the incident, or spoke about it even some days after it occured. Another point worth to notice is that the intervals are very unbalanced: the life event almost always falls near an extreme of the time range, or right on it. This means that users usually talk about the event or only before it happens, or only later. In particular, in births cases is most common to announce the event with some advance, while with weddings is usual to post something about it in the days immediately following.

On wrong intervals the distance in days between the date of the real event and the nearest extreme of the interval was computed, as shown in Table~\ref{tab:wrongintervals}. In four errors out of five, the life event was detected before the incident: this means that these users announced the coming event in advance, without posting anything right after it occured. The giant error in a \texttt{Getting married} detection, 276 days of distance between the result and the truth, is a particular case: the user wrote this post to announce the birth of his child: \textit{"My strong wife Patricia and me are glad to introduce you Gabriel!"}\footnote{\url{twitter.com/manuelquinziato/status/763056696180834305}}, and the classifier marked it as a post belonging to a wedding, instead of to a birth. In this case the analysis of the attached image could have helped.

In conclusion, so much uncertainty is caused by the subjectivity with which people decide to share a personal life event on social networks. As highlighted by the tests, several users share something only when the event is upcoming, while some others make announcements with some advance. 

